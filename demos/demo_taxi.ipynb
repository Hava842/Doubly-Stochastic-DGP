{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massive scale regression\n",
    "\n",
    "**Warning:** this dataset will occupy 76GB on space your disk. Check that the download location is appropriate for data this size. You will also need a machine with about 16GB of memory to run this code. \n",
    "\n",
    "The taxi data set consists of 1.21 billion yellow taxi journeys in New York. We obtained the data from http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\n",
    "\n",
    "The processing was as follows:\n",
    "- We extracted the following features: time of day; day of the week; day of the month; month; pick-up latitude and longitude; drop-off latitude and longitude; travel distance; journey time (the target)\n",
    "- We discarded journeys that are less than 10 s or greater than 5 h, or start/end outside the New York region, which we judge to have squared distance less than $5^o$ from the centre of New York\n",
    "- As we read in the data we calculated $\\sum x$ and $\\sum x^2$. These are in the file `taxi_data_stats.p`. We use these for normalizing the data. In the paper we normalise the outputs and restore the scaling, but here we use a mean function and set the variance accordingly. \n",
    "- We shuffled the entire data set (we used a machine with 224GB of memory to do this) and then split the data into 101 files each with $10^7$ lines. We use the first 100 chunks for training and final chunk for testing \n",
    "\n",
    "To use this data set managably on a standard machine we read in two chunks at a time, the second loading asynchronously as the first chunk is used for training. We have a special `DataHolder` class for this  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from GPflow.likelihoods import Gaussian\n",
    "from GPflow.kernels import RBF, White\n",
    "from GPflow.mean_functions import Constant, Zero\n",
    "from GPflow.svgp import SVGP\n",
    "from GPflow.param import DataHolder, Parentable\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from scipy.stats import norm\n",
    "from get_data import get_taxi_data, get_taxi_stats\n",
    "\n",
    "from threading import Thread\n",
    "from Queue import Queue\n",
    "\n",
    "from dgp import DGP\n",
    "import time\n",
    "\n",
    "data_path = '/mnt/' # requires 76GB of free space. Download size is approx 28GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wrapper(func, arg, queue):\n",
    "    queue.put(func(arg))\n",
    "\n",
    "class TaxiData(DataHolder):\n",
    "    def __init__(self, minibatch_size=10000):\n",
    "        Parentable.__init__(self)\n",
    "        self._shape = [minibatch_size, 10]\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.counter = 0\n",
    "        self.chunk_counter = 0\n",
    "        \n",
    "        self.num_data = int(10**9)\n",
    "        self.chunk_size = int(10**7)\n",
    "        self.num_chunks = int(self.num_data/self.chunk_size)\n",
    "\n",
    "        self.X_mean, self.X_std = get_taxi_stats(data_path=data_path) \n",
    "\n",
    "        self.current_chunk = self.get_chunk(0) # get first chunk\n",
    "        self.chunk_counter += 1\n",
    "        self.start_get_chunk(self.chunk_counter) # start loading next one\n",
    "        \n",
    "    \n",
    "    def start_get_chunk(self, i):\n",
    "        self.next_chunk_queued = Queue() \n",
    "        Thread(target=wrapper, args=(self.get_chunk, i, \n",
    "                                     self.next_chunk_queued)).start()\n",
    "    \n",
    "    def get_chunk(self, i):\n",
    "        return self.whiten(get_taxi_data(i, data_path=data_path))\n",
    "    \n",
    "    def whiten(self, data):\n",
    "        return (data - self.X_mean)/self.X_std\n",
    "    \n",
    "    def _get_type(self):\n",
    "        return np.float64\n",
    "\n",
    "    def make_tf_array(self):\n",
    "        self._tf_array = tf.placeholder(dtype=self._get_type(),\n",
    "                                        shape=[None, self._shape[1]],\n",
    "                                        name=self.name)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        raise NotImplementedError #can't access this data directly \n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        return np.prod(self.shape)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "\n",
    "    def __str__(self, prepend='Data:'):\n",
    "        return prepend + \\\n",
    "               '\\033[1m' + self.name + '\\033[0m' + \\\n",
    "               '\\n Data much too large to print!' + \\\n",
    "               '\\n First 10 lines of current chunk are: ' + \\\n",
    "                '\\n' + str(self.current_chunk[:10, :])\n",
    "                \n",
    "    def update_feed_dict(self, key_dict, feed_dict):\n",
    "        if self.counter + self.minibatch_size > self.chunk_size:\n",
    "            self.current_chunk = self.next_chunk_queued.get()\n",
    "            self.chunk_counter = (self.chunk_counter + 1) % self.num_chunks\n",
    "            self.start_get_chunk(self.chunk_counter)\n",
    "            self.counter = 0     \n",
    "       \n",
    "        start = self.counter\n",
    "        end = self.counter + self.minibatch_size\n",
    "        \n",
    "        self.counter += self.minibatch_size\n",
    "        \n",
    "        feed_dict[key_dict[self]] = self.current_chunk[start:end, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the $10^6$ from the first chunk to find the initial inducing locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_data = TaxiData(minibatch_size=10000)\n",
    "Z = kmeans2(taxi_data.current_chunk[:int(1e6), :-1], 100, minit='points')[0]\n",
    "\n",
    "Y_std = taxi_data.X_std[:, -1]\n",
    "\n",
    "test_data = taxi_data.get_chunk(101)\n",
    "\n",
    "Ns = int(1e6)\n",
    "Xs, Ys = test_data[:Ns, :-1], test_data[:Ns, -1, None]\n",
    "\n",
    "num_val = int(1e5) # some validation data (from training set) to see what's going on\n",
    "X_val, Y_val = taxi_data.current_chunk[:num_val, :-1], taxi_data.current_chunk[:num_val, -1, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some tools to assess the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_assess(model, assess_model, X, Y):\n",
    "    n_batches = max(int(X.shape[0]/1000.), 1)\n",
    "    lik, sq_diff = [], []\n",
    "    for X_batch, Y_batch in zip(np.array_split(X, n_batches), np.array_split(Y, n_batches)):\n",
    "        l, sq = assess_model(model, X_batch, Y_batch)\n",
    "        lik.append(l)\n",
    "        sq_diff.append(sq)\n",
    "    lik = np.concatenate(lik, 0)\n",
    "    sq_diff = np.array(np.concatenate(sq_diff, 0), dtype=float)\n",
    "    return np.average(lik), np.average(sq_diff)**0.5\n",
    "\n",
    "def assess_single_layer(model, X_batch, Y_batch):\n",
    "    mean, var = model.predict_y(X_batch)\n",
    "    lik = norm.logpdf(Y_std*Y_batch, loc=Y_std*mean, scale=Y_std*var**0.5)\n",
    "    sq_diff = ((mean - Y_batch)**2)\n",
    "    return lik, sq_diff * Y_std**2\n",
    "\n",
    "S = 100\n",
    "def assess_sampled(model, X_batch, Y_batch):\n",
    "    mean_samples, var_samples = model.predict_y(X_batch, S)\n",
    "    Y = Y_batch[None, :, :] * np.ones(S, None, None)\n",
    "    lik = np.average(norm.logpdf(Y_std*Y, loc=Y_std*mean_samples, scale=Y_std*var_samples**0.5), 0)\n",
    "    mean = np.average(mean_samples, 0)\n",
    "    sq_diff = ((mean - Y_batch)**2)\n",
    "    return lik, sq_diff * Y_std**2\n",
    "\n",
    "\n",
    "class CB(object):\n",
    "    def __init__(self, model, assess_model):\n",
    "        self.model = model\n",
    "        self.assess_model = assess_model\n",
    "        self.i = 0\n",
    "        self.t = time.time()\n",
    "        self.train_time = 0\n",
    "        self.ob = []\n",
    "        self.train_lik = []\n",
    "        self.train_rmse = []\n",
    "    def cb(self, x):\n",
    "        self.i += 1\n",
    "        if self.i % 10000 == 0:\n",
    "            # time how long we've be training \n",
    "            self.train_time += time.time() - self.t\n",
    "            self.t = time.time()\n",
    "            \n",
    "            # assess the model on the training data\n",
    "            self.model.set_state(x)\n",
    "            lik, rmse = batch_assess(self.model, self.assess_model, X_val, Y_val)\n",
    "            self.train_lik.append(lik)\n",
    "            self.train_rmse.append(rmse)\n",
    "            \n",
    "            # calculate the objective, averaged over 100 samples \n",
    "            ob = 0\n",
    "            num_samples = 100\n",
    "            for _ in range(num_samples):\n",
    "                ob += self.model.compute_log_likelihood()/float(num_samples)\n",
    "            self.ob.append(ob)\n",
    "            \n",
    "            st = 'it: {}, ob: {:.1f}, train lik: {:.4f}, train rmse {:.4f}'\n",
    "            print st.format(self.i, ob, lik, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a single layer model we need to slightly modify the base SVGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MassiveDataSVGP(SVGP):\n",
    "    def __init__(self, dataholder, kernel, likelihood, Z, \n",
    "                 q_diag=False, whiten=True, num_latent=1, mean_function=Zero()):\n",
    "        D = dataholder.shape[1] - 1\n",
    "        SVGP.__init__(self, np.zeros((1, D)), np.zeros((1, 1)), kernel, likelihood, Z, \n",
    "                      q_diag=q_diag, whiten=whiten, num_latent=num_latent)\n",
    "        del self.X\n",
    "        del self.Y\n",
    "        self.dataholder = dataholder\n",
    "        self.num_data = dataholder.num_data\n",
    "    \n",
    "    def build_likelihood(self):\n",
    "        self.X = self.dataholder[:, :-1]\n",
    "        self.Y = self.dataholder[:, -1, None]        \n",
    "        return SVGP.build_likelihood(self)\n",
    "\n",
    "class TaxiSVGP(MassiveDataSVGP):\n",
    "    def __init__(self, data=taxi_data):\n",
    "        D = data.shape[1] - 1\n",
    "        MassiveDataSVGP.__init__(self, data, RBF(D, ARD=True), Gaussian(), Z.copy())\n",
    "        self.likelihood.variance = 0.01\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the sgp for 2 epoch (which is $2\\times10^5$ iterations, since the minibatch size is $10^4$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_iterations = int(2e5)\n",
    "\n",
    "def train(model, name, assess, Xs, Ys):\n",
    "    cb = CB(model, assess)\n",
    "    model.optimize(tf.train.AdamOptimizer(), # 1e-3 is the default here\n",
    "                   maxiter=num_iterations,\n",
    "                   callback=cb.cb)\n",
    "\n",
    "    l, rmse = batch_assess(model, assess, Xs, Ys)\n",
    "    print '\\n{} test lik {:.4f}, test rmse {:.4f}. Train time: {:.4f}'.format(name, l, rmse, cb.train_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 10000, ob: -1008499961.5, train lik: -7.2885, train rmse 367.3656\n",
      "it: 20000, ob: -831362087.6, train lik: -7.1444, train rmse 313.0104\n",
      "it: 30000, ob: -797394924.7, train lik: -7.1372, train rmse 309.7279\n",
      "it: 40000, ob: -794511397.8, train lik: -7.1330, train rmse 308.1813\n",
      "it: 50000, ob: -790479894.8, train lik: -7.1304, train rmse 306.9690\n",
      "it: 60000, ob: -791620609.2, train lik: -7.1316, train rmse 307.4513\n",
      "it: 70000, ob: -789349648.5, train lik: -7.1306, train rmse 307.0009\n",
      "it: 80000, ob: -794149819.7, train lik: -7.1322, train rmse 307.2403\n",
      "it: 90000, ob: -782359641.2, train lik: -7.1275, train rmse 305.6874\n",
      "it: 100000, ob: -790395807.3, train lik: -7.1255, train rmse 305.0272\n",
      "it: 110000, ob: -788365500.9, train lik: -7.1284, train rmse 305.9350\n",
      "it: 120000, ob: -784702584.6, train lik: -7.1249, train rmse 304.8569\n",
      "it: 130000, ob: -792324020.9, train lik: -7.1257, train rmse 304.9313\n",
      "it: 140000, ob: -788712499.5, train lik: -7.1229, train rmse 303.8370\n",
      "it: 150000, ob: -781951105.0, train lik: -7.1222, train rmse 303.6282\n",
      "it: 160000, ob: -787233404.0, train lik: -7.1204, train rmse 303.0855\n",
      "it: 170000, ob: -772700506.0, train lik: -7.1241, train rmse 304.1782\n",
      "it: 180000, ob: -778775410.0, train lik: -7.1216, train rmse 303.2503\n",
      "it: 190000, ob: -781359753.6, train lik: -7.1221, train rmse 303.4981\n",
      "it: 200000, ob: -789949933.2, train lik: -7.1258, train rmse 304.5028\n",
      "\n",
      "sgp test lik -7.1265, test rmse 304.4952. Train time: 3071.6764\n"
     ]
    }
   ],
   "source": [
    "svgp= TaxiSVGP(taxi_data)\n",
    "train(svgp, 'sgp', assess_single_layer,  Xs, Ys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can modify the DGP class to work with the dataholder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MassiveDataDGP(DGP):\n",
    "    def __init__(self, dataholder, Z, kernels, likelihood, num_latent_Y=1, mean_function=Zero()):\n",
    "        DGP.__init__(self, np.zeros((1, 9)), np.zeros((1, 1)), Z, kernels, likelihood, \n",
    "                     num_latent_Y=num_latent_Y, mean_function=mean_function)\n",
    "        del self.X\n",
    "        del self.Y\n",
    "        self.dataholder = dataholder\n",
    "        self.num_data = dataholder.num_data\n",
    "        \n",
    "    def build_likelihood(self):\n",
    "        self.X = self.dataholder[:, :-1]\n",
    "        self.Y = self.dataholder[:, -1, None]        \n",
    "        return DGP.build_likelihood(self)\n",
    "\n",
    "class TaxiDGP(MassiveDataDGP):\n",
    "    def __init__(self, L, data=taxi_data):\n",
    "        D = data.shape[1] - 1\n",
    "        kernels = []\n",
    "        for _ in range(L):\n",
    "            kernels.append(RBF(D, ARD=True))\n",
    "            \n",
    "        MassiveDataDGP.__init__(self, data, Z.copy(), kernels, Gaussian())\n",
    "        self.likelihood.variance = 0.01\n",
    "        \n",
    "        for layer in self.layers[:-1]:\n",
    "            layer.q_sqrt = layer.q_sqrt.value * 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a 2 layer DGP model, with the RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 10000, ob: -831684006.7, train lik: -7.1327, train rmse 308.8983\n",
      "it: 20000, ob: -789538389.3, train lik: -7.1273, train rmse 303.8887\n",
      "it: 30000, ob: -775163174.0, train lik: -7.1124, train rmse 299.2709\n",
      "it: 40000, ob: -757446849.7, train lik: -7.1032, train rmse 296.7112\n",
      "it: 50000, ob: -762769346.6, train lik: -7.1014, train rmse 296.1190\n",
      "it: 60000, ob: -746506036.4, train lik: -7.0982, train rmse 294.9962\n",
      "it: 70000, ob: -755334438.2, train lik: -7.1011, train rmse 295.7932\n",
      "it: 80000, ob: -739731776.6, train lik: -7.0885, train rmse 291.9556\n",
      "it: 90000, ob: -745961139.5, train lik: -7.0856, train rmse 290.9572\n",
      "it: 100000, ob: -746717410.4, train lik: -7.0842, train rmse 290.8403\n",
      "it: 110000, ob: -732544156.2, train lik: -7.0822, train rmse 290.0791\n",
      "it: 120000, ob: -725234954.4, train lik: -7.0755, train rmse 288.3537\n",
      "it: 130000, ob: -723717283.0, train lik: -7.0779, train rmse 288.7962\n",
      "it: 140000, ob: -736924627.5, train lik: -7.0736, train rmse 287.5269\n",
      "it: 150000, ob: -728163835.1, train lik: -7.0742, train rmse 287.5801\n",
      "it: 160000, ob: -724133137.8, train lik: -7.0714, train rmse 286.7266\n",
      "it: 170000, ob: -727434310.1, train lik: -7.0759, train rmse 288.2789\n",
      "it: 180000, ob: -721099761.8, train lik: -7.0707, train rmse 286.4507\n",
      "it: 190000, ob: -728368765.6, train lik: -7.0738, train rmse 287.3034\n",
      "it: 200000, ob: -716470776.0, train lik: -7.0712, train rmse 286.5033\n",
      "\n",
      "dgp 2 test lik -7.0713, test rmse 286.3280. Train time: 12577.6848\n"
     ]
    }
   ],
   "source": [
    "train(TaxiDGP(2), 'dgp 2', assess_sampled, Xs, Ys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the three layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 10000, ob: -833053149.1, train lik: -7.1659, train rmse 312.8360\n",
      "it: 20000, ob: -752739757.8, train lik: -7.1060, train rmse 295.0622\n",
      "it: 30000, ob: -736369740.7, train lik: -7.0865, train rmse 289.6944\n",
      "it: 40000, ob: -730033940.3, train lik: -7.0770, train rmse 286.8153\n",
      "it: 50000, ob: -729822623.7, train lik: -7.0781, train rmse 286.8667\n",
      "it: 60000, ob: -717538402.5, train lik: -7.0711, train rmse 284.8011\n",
      "it: 70000, ob: -718024639.9, train lik: -7.0657, train rmse 283.3281\n",
      "it: 80000, ob: -721011115.5, train lik: -7.0617, train rmse 282.1505\n",
      "it: 90000, ob: -709860263.0, train lik: -7.0624, train rmse 282.2137\n",
      "it: 100000, ob: -708479977.7, train lik: -7.0624, train rmse 282.2003\n",
      "it: 110000, ob: -694890143.9, train lik: -7.0579, train rmse 280.9410\n",
      "it: 120000, ob: -699679708.3, train lik: -7.0562, train rmse 280.4285\n",
      "it: 130000, ob: -701069667.8, train lik: -7.0558, train rmse 280.2378\n",
      "it: 140000, ob: -708757433.3, train lik: -7.0559, train rmse 280.3539\n",
      "it: 150000, ob: -707762715.6, train lik: -7.0630, train rmse 282.3146\n",
      "it: 160000, ob: -687601372.5, train lik: -7.0555, train rmse 280.2584\n",
      "it: 170000, ob: -712316844.8, train lik: -7.0633, train rmse 282.5572\n",
      "it: 180000, ob: -690979719.5, train lik: -7.0516, train rmse 279.2041\n",
      "it: 190000, ob: -693574106.8, train lik: -7.0516, train rmse 279.1777\n",
      "it: 200000, ob: -686303868.6, train lik: -7.0506, train rmse 278.8522\n",
      "\n",
      "dgp 3 test lik -7.0491, test rmse 278.7810. Train time: 20558.9546\n"
     ]
    }
   ],
   "source": [
    "train(TaxiDGP(3) , 'dgp 3', assess_sampled, Xs, Ys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 10000, ob: -905088531.7, train lik: -7.2492, train rmse 334.4848\n",
      "it: 20000, ob: -768081018.5, train lik: -7.1158, train rmse 295.1995\n",
      "it: 30000, ob: -742491827.8, train lik: -7.0965, train rmse 290.1173\n",
      "it: 40000, ob: -720274156.0, train lik: -7.0800, train rmse 285.6062\n",
      "it: 50000, ob: -725725617.7, train lik: -7.0784, train rmse 285.3012\n",
      "it: 60000, ob: -713218710.4, train lik: -7.0698, train rmse 282.9368\n",
      "it: 70000, ob: -704333463.9, train lik: -7.0621, train rmse 281.1464\n",
      "it: 80000, ob: -708119434.7, train lik: -7.0658, train rmse 282.2222\n",
      "it: 90000, ob: -704155779.7, train lik: -7.0585, train rmse 280.3807\n",
      "it: 100000, ob: -697184788.7, train lik: -7.0568, train rmse 279.9439\n",
      "it: 110000, ob: -688436584.1, train lik: -7.0523, train rmse 278.8712\n",
      "it: 120000, ob: -691448553.3, train lik: -7.0569, train rmse 280.3083\n",
      "it: 130000, ob: -698351611.3, train lik: -7.0508, train rmse 278.6431\n",
      "it: 140000, ob: -702594860.1, train lik: -7.0481, train rmse 277.8065\n",
      "it: 150000, ob: -684664023.1, train lik: -7.0477, train rmse 277.7713\n",
      "it: 160000, ob: -679423251.3, train lik: -7.0472, train rmse 277.6540\n",
      "it: 170000, ob: -685089096.9, train lik: -7.0451, train rmse 277.0175\n",
      "it: 180000, ob: -679104874.7, train lik: -7.0436, train rmse 276.6340\n",
      "it: 190000, ob: -690444167.2, train lik: -7.0440, train rmse 276.8279\n",
      "it: 200000, ob: -677280089.4, train lik: -7.0415, train rmse 276.1599\n",
      "\n",
      "dgp 4 test lik -7.0403, test rmse 276.1547. Train time: 28541.6200\n"
     ]
    }
   ],
   "source": [
    "train(TaxiDGP(4) , 'dgp 4', assess_sampled, Xs, Ys)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
